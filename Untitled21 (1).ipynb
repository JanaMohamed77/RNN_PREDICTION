{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ebfe40-722f-45d7-8446-142694004b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: [8.78653819]\n",
      "Epoch 500, Loss: [1.57657521]\n",
      "Epoch 1000, Loss: [0.17882876]\n",
      "Epoch 1500, Loss: [0.04759607]\n",
      "Epoch 2000, Loss: [0.0266513]\n",
      "Epoch 2500, Loss: [0.01826221]\n",
      "Epoch 3000, Loss: [0.01381411]\n",
      "Epoch 3500, Loss: [0.01108126]\n",
      "Epoch 4000, Loss: [0.00924035]\n",
      "Epoch 4500, Loss: [0.00791939]\n",
      "Prediction for ['I', 'love', 'deep']: learning\n",
      "Prediction for ['machine', 'learning', 'is']: fun\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# إعداد البيانات\n",
    "data = [\n",
    "    (['I', 'love', 'deep'], 'learning'),\n",
    "    (['deep', 'learning', 'is'], 'awesome'),\n",
    "    (['I', 'enjoy', 'machine'], 'learning'),\n",
    "    (['machine', 'learning', 'is'], 'fun')\n",
    "]\n",
    "\n",
    "# بناء قاموس الكلمات\n",
    "words = list(set([word for sentence, target in data for word in sentence] + [target for _, target in data]))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(words)\n",
    "\n",
    "# إعداد البراميترز\n",
    "hidden_size = 16  # حجم الذاكرة المخفية\n",
    "learning_rate = 0.01\n",
    "\n",
    "# الوزنات\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # bias hidden\n",
    "by = np.zeros((vocab_size, 1))   # bias output\n",
    "\n",
    "# تحويل الكلمات لـ One-hot\n",
    "def word_to_one_hot(word):\n",
    "    one_hot = np.zeros((vocab_size, 1))\n",
    "    one_hot[word_to_idx[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Softmax\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "# التدريب\n",
    "for epoch in range(5000):\n",
    "    total_loss = 0\n",
    "    for sentence, target in data:\n",
    "        # Forward pass\n",
    "        hs = {}\n",
    "        hs[-1] = np.zeros((hidden_size, 1))\n",
    "\n",
    "        for t in range(len(sentence)):\n",
    "            xt = word_to_one_hot(sentence[t])\n",
    "            hs[t] = np.tanh(np.dot(Wxh, xt) + np.dot(Whh, hs[t-1]) + bh)\n",
    "\n",
    "        # Output layer\n",
    "        y = np.dot(Why, hs[len(sentence)-1]) + by\n",
    "        p = softmax(y)\n",
    "\n",
    "        # Loss (cross-entropy)\n",
    "        target_idx = word_to_idx[target]\n",
    "        loss = -np.log(p[target_idx])\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        # Gradients initialization\n",
    "        dWhy = np.zeros_like(Why)\n",
    "        dWxh = np.zeros_like(Wxh)\n",
    "        dWhh = np.zeros_like(Whh)\n",
    "        dbh = np.zeros_like(bh)\n",
    "        dby = np.zeros_like(by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "        # Output layer gradient\n",
    "        dy = np.copy(p)\n",
    "        dy[target_idx] -= 1  # derivative of loss wrt softmax input\n",
    "\n",
    "        dWhy += np.dot(dy, hs[len(sentence)-1].T)\n",
    "        dby += dy\n",
    "\n",
    "        # Backprop through time\n",
    "        for t in reversed(range(len(sentence))):\n",
    "            dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh  # tanh derivative\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, word_to_one_hot(sentence[t]).T)\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "            dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "        # Clip to prevent exploding gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        # Update weights\n",
    "        Wxh -= learning_rate * dWxh\n",
    "        Whh -= learning_rate * dWhh\n",
    "        Why -= learning_rate * dWhy\n",
    "        bh -= learning_rate * dbh\n",
    "        by -= learning_rate * dby\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss}')\n",
    "\n",
    "# Prediction function\n",
    "def predict(sentence):\n",
    "    hs = {}\n",
    "    hs[-1] = np.zeros((hidden_size, 1))\n",
    "    for t in range(len(sentence)):\n",
    "        xt = word_to_one_hot(sentence[t])\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xt) + np.dot(Whh, hs[t-1]) + bh)\n",
    "\n",
    "    y = np.dot(Why, hs[len(sentence)-1]) + by\n",
    "    p = softmax(y)\n",
    "    idx = np.argmax(p)\n",
    "    return idx_to_word[idx]\n",
    "\n",
    "# اختبار الموديل\n",
    "print(\"Prediction for ['I', 'love', 'deep']:\", predict(['I', 'love', 'deep']))\n",
    "print(\"Prediction for ['machine', 'learning', 'is']:\", predict(['machine', 'learning', 'is']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f1914-e75a-4d48-aaae-79d320e46d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
